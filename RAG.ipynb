{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karan\\AppData\\Local\\Temp\\ipykernel_6804\\923350730.py:44: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(r\"C:\\Users\\karan\\Documents\\Projects\\Gmail\\nvidia-ai-aerial-faq.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=7500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings using Ollama\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize Ollama model\n",
    "llm = Ollama(model=\"llama3.2:latest\")\n",
    "\n",
    "# Create QA chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"vectorstore_directory\"  # Specify directory here\n",
    ")\n",
    "\n",
    "# Save to disk\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load the saved vector store\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"vectorstore_directory\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Initialize Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\")\n",
    "\n",
    "# Create the QA chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Query the document\n",
    "chat_history = []\n",
    "query = \"What is this document about?\"\n",
    "result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "\n",
    "# For follow-up questions, update chat history\n",
    "chat_history.append((query, result['answer']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is about NVIDIA AI Aerial FAQ. It's a set of Questions and Answers regarding the NVIDIA AI Aerial system, its deployment, and various use cases, including ARC-OTA and OAI stack.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Any, Optional, List\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "# Initialize Cerebras client\n",
    "client = Cerebras(\n",
    "    api_key=os.getenv(\"CEREBRAS_API_KEY\", \"csk-e2e8kypw838rwmpjxd9nx2vn5jrertm339fnrcnt9c6p8hmx\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class CloudLLM(LLM):\n",
    "    _client: Any = PrivateAttr()\n",
    "    model_name: str\n",
    "\n",
    "    def __init__(self, client: Any, model_name: str):\n",
    "        super().__init__(model_name=model_name)\n",
    "        self._client = client\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response = self._client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=self.model_name,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"cloud_llm\"\n",
    "\n",
    "\n",
    "# Load the saved vector store\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"vectorstore_directory\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Initialize the custom cloud LLM\n",
    "cloud_llm = CloudLLM(client=client, model_name=\"llama3.1-8b\")\n",
    "\n",
    "# Create the conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cloud_llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Query the document\n",
    "chat_history = []\n",
    "query = \"What is this document about?\"\n",
    "result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "\n",
    "# Update chat history for follow-up questions\n",
    "chat_history.append((query, result['answer']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Email Details:\n",
      "{\n",
      "    \"Subject\": \"Test Inquiry\",\n",
      "    \"From\": \"Karan Vora <kv2154@nyu.edu>\",\n",
      "    \"To\": \"Karan Vora <karanvora210@gmail.com>\",\n",
      "    \"Body\": \"I want to inquiry about the specifications of the Nvidia Ariel Products and\\r\\nits use-cases.\\r\\n\\r\\nBest Regards,\\r\\nKaran Vora\\r\\nNew York University Class of 2024\\r\\nMS Computer Engineering\\r\\nkv2154@nyu.edu\\r\\nkaranvora210@gmail.com\\r\\nContact Number: +1 929-930-0027\\r\\nLinkedin <https://www.linkedin.com/in/karan-vora-574961188/>\\r\\n\"\n",
      "}\n",
      "Classification: Inquiry\n",
      "Email classification saved to 'classified_email.json'.\n",
      "Generated RAG Response: Dear Karan Vora,\n",
      "\n",
      "Thank you for your inquiry about the specifications of the NVIDIA Aerial products and its use-cases. I'm happy to help answer your questions based on the provided information.\n",
      "\n",
      "NVIDIA AI Aerial is a platform designed for the development of software-defined cellular networks and supports various 5G use-cases. However, the information provided does not mention any specific product specifications, such as the type of hardware or software required to run the platform.\n",
      "\n",
      "Regarding its use-cases, NVIDIA AI Aerial can be used for 5G network simulations, including channel estimation and physical layer modeling. The platform supports various 5G waveforms and can be used to develop software-defined radio (SDR) systems.\n",
      "\n",
      "Some specific details on the platform's capabilities and use-cases are:\n",
      "\n",
      "- Supports 5G waveforms, as mentioned in question 1.4, and has the potential to support other waveforms like 4G and DVB-S2 in the future.\n",
      "- Can simulate metamaterial surfaces and other applications, as mentioned in question 2.4.\n",
      "- Has a differentiable model to tune and minimize discrepancies against field measurements, which can be used at different frequency bands, including mmWave.\n",
      "- Supports the use of pilot symbols/reference signals for ML-based channel estimation, as mentioned in question 2.5.\n",
      "- The underlying framework for AI is Torched, while for PHY and MAC is NVIDIA cuPHY and cuMAC, respectively. The UI is based on NVIDIA Omniverse KIT.\n",
      "- Sionna allows for the creation of data-plane designs and is open-source for non-commercial use.\n",
      "- The current runtime is 500ms per slot per cell on L40S, and the target is roughly 1000x faster on a single L40S.\n",
      "\n",
      "To learn more about NVIDIA AI Aerial and its use-cases, it is recommended that you refer to the official documentation and contact NVIDIA support for further information.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "Reply sent to thread ID: 194701c2797f1995\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import base64\n",
    "import json\n",
    "from email.mime.text import MIMEText\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Any, Optional, List\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "# -------------------- Gmail API Setup --------------------\n",
    "\n",
    "# Define the scope for Gmail API\n",
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/gmail.readonly',\n",
    "    'https://www.googleapis.com/auth/gmail.send'\n",
    "]\n",
    "\n",
    "def authenticate_gmail():\n",
    "    \"\"\"Authenticate the user and return the Gmail service.\"\"\"\n",
    "    creds = None\n",
    "    # Load credentials from token.pickle if it exists\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If no valid credentials, prompt the user to log in\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secret.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0, access_type='offline', prompt='consent')\n",
    "        # Save the credentials for future use\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    # Build the Gmail service\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "    return service\n",
    "\n",
    "def get_latest_email(service):\n",
    "    \"\"\"Retrieve the most recent email from the user's Primary inbox.\"\"\"\n",
    "    try:\n",
    "        # Fetch the latest email from the Primary category\n",
    "        results = service.users().messages().list(\n",
    "            userId='me',\n",
    "            maxResults=1,\n",
    "            q=\"category:primary\"\n",
    "        ).execute()\n",
    "        messages = results.get('messages', [])\n",
    "        if not messages:\n",
    "            print(\"No messages found in the Primary category.\")\n",
    "            return None\n",
    "        # Get the message details\n",
    "        message = service.users().messages().get(\n",
    "            userId='me',\n",
    "            id=messages[0]['id'],\n",
    "            format='full'\n",
    "        ).execute()\n",
    "        return message\n",
    "    except Exception as error:\n",
    "        print(f\"An error occurred while fetching the latest email: {error}\")\n",
    "        return None\n",
    "\n",
    "def decode_message_body(encoded_body):\n",
    "    \"\"\"Decode the base64url-encoded message body.\"\"\"\n",
    "    try:\n",
    "        # Add padding if necessary\n",
    "        padding = 4 - (len(encoded_body) % 4)\n",
    "        if padding:\n",
    "            encoded_body += \"=\" * padding\n",
    "        decoded_bytes = base64.urlsafe_b64decode(encoded_body)\n",
    "        return decoded_bytes.decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding message body: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_email_details(message):\n",
    "    \"\"\"Extract details from the email message.\"\"\"\n",
    "    headers = message['payload'].get('headers', [])\n",
    "    subject = next((header['value'] for header in headers if header['name'] == 'Subject'), \"No Subject\")\n",
    "    sender = next((header['value'] for header in headers if header['name'] == 'From'), \"Unknown Sender\")\n",
    "    recipient = next((header['value'] for header in headers if header['name'] == 'To'), \"Unknown Recipient\")\n",
    "    body = \"\"\n",
    "    if 'parts' in message['payload']:\n",
    "        for part in message['payload']['parts']:\n",
    "            if part['mimeType'] == 'text/plain' and 'data' in part['body']:\n",
    "                body = decode_message_body(part['body']['data'])\n",
    "                break\n",
    "    elif 'body' in message['payload'] and 'data' in message['payload']['body']:\n",
    "        body = decode_message_body(message['payload']['body']['data'])\n",
    "    return {\n",
    "        \"Subject\": subject,\n",
    "        \"From\": sender,\n",
    "        \"To\": recipient,\n",
    "        \"Body\": body\n",
    "    }\n",
    "\n",
    "def create_reply_message(service, original_message, reply_body):\n",
    "    \"\"\"Create a reply message for the given original email.\"\"\"\n",
    "    try:\n",
    "        # Extract necessary details from the original message\n",
    "        thread_id = original_message['threadId']\n",
    "        message_id = None\n",
    "        headers = original_message['payload'].get('headers', [])\n",
    "        for header in headers:\n",
    "            if header['name'] == 'Message-ID':\n",
    "                message_id = header['value']\n",
    "            elif header['name'] == 'From':\n",
    "                sender = header['value']\n",
    "            elif header['name'] == 'To':\n",
    "                recipient = header['value']\n",
    "            elif header['name'] == 'Subject':\n",
    "                subject = header['value']\n",
    "\n",
    "        # Create the reply email\n",
    "        reply = MIMEText(reply_body)\n",
    "        reply['To'] = sender  # Reply to the original sender\n",
    "        reply['From'] = recipient  # Your email address\n",
    "        reply['Subject'] = f\"Re: {subject}\"\n",
    "        reply['In-Reply-To'] = message_id\n",
    "        reply['References'] = message_id\n",
    "\n",
    "        # Encode the message\n",
    "        raw_message = base64.urlsafe_b64encode(reply.as_bytes()).decode()\n",
    "\n",
    "        # Create the message object with threadId\n",
    "        return {\n",
    "            'raw': raw_message,\n",
    "            'threadId': thread_id\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(f\"An error occurred while creating the reply message: {error}\")\n",
    "        return None\n",
    "\n",
    "def send_reply(service, original_message, reply_body):\n",
    "    \"\"\"Send a reply to an email message.\"\"\"\n",
    "    try:\n",
    "        reply = create_reply_message(service, original_message, reply_body)\n",
    "        if reply:\n",
    "            sent_message = service.users().messages().send(userId='me', body=reply).execute()\n",
    "            print(f\"Reply sent to thread ID: {sent_message['threadId']}\")\n",
    "        else:\n",
    "            print(\"Failed to create reply message.\")\n",
    "    except Exception as error:\n",
    "        print(f\"An error occurred while sending the reply: {error}\")\n",
    "\n",
    "# -------------------- Cerebras API Setup --------------------\n",
    "\n",
    "# Initialize the Cerebras client\n",
    "CEREBRAS_API_KEY = os.getenv(\"CEREBRAS_API_KEY\", \"csk-e2e8kypw838rwmpjxd9nx2vn5jrertm339fnrcnt9c6p8hmx\")  # Replace with your actual API key or set as environment variable\n",
    "client = Cerebras(api_key=CEREBRAS_API_KEY)\n",
    "\n",
    "def classify_email_body(email_body):\n",
    "    \"\"\"Classify the email body as 'FAQ', 'Inquiry', or 'Other'.\"\"\"\n",
    "    try:\n",
    "        # Define the system prompt\n",
    "        system_prompt = \"\"\"You are an email classifier. Classify the following email body into one of the following categories: 'FAQ', 'Inquiry', or 'Other'. Respond with the classification only.\"\"\"\n",
    "        \n",
    "        # Create the completion request\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b\",  # Specify the model you wish to use\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": email_body}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=10,\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "\n",
    "        # Extract and return the classification\n",
    "        parsed_content = completion.choices[0].message.content.strip()\n",
    "        return parsed_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying email body: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# -------------------- RAG (Retrieval-Augmented Generation) Setup --------------------\n",
    "\n",
    "class CloudLLM(LLM):\n",
    "    _client: Any = PrivateAttr()\n",
    "    model_name: str\n",
    "\n",
    "    def __init__(self, client: Any, model_name: str):\n",
    "        super().__init__(model_name=model_name)\n",
    "        self._client = client\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        try:\n",
    "            response = self._client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=self.model_name,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response with LLM: {e}\")\n",
    "            return \"I'm sorry, I couldn't process your request at this time.\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"cloud_llm\"\n",
    "\n",
    "def setup_rag():\n",
    "    \"\"\"Initialize the RAG system.\"\"\"\n",
    "    try:\n",
    "        # Load the saved vector store\n",
    "        embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=\"vectorstore_directory\",  # Ensure this directory exists and is populated\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        # Initialize the custom cloud LLM\n",
    "        cloud_llm = CloudLLM(client=client, model_name=\"llama3.1-8b\")  # Ensure the model name is correct and available\n",
    "\n",
    "        # Create the conversational retrieval chain\n",
    "        qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=cloud_llm,\n",
    "            retriever=vectorstore.as_retriever()\n",
    "        )\n",
    "\n",
    "        return qa_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up RAG system: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_rag_response(qa_chain, query, chat_history=None):\n",
    "    \"\"\"Generate a response using the RAG system.\"\"\"\n",
    "    try:\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "        result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "        return result['answer']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating RAG response: {e}\")\n",
    "        return \"I'm sorry, I couldn't find the information you're looking for.\"\n",
    "\n",
    "# -------------------- Main Workflow --------------------\n",
    "\n",
    "def main():\n",
    "    # Step 1: Authenticate and build the Gmail service\n",
    "    service = authenticate_gmail()\n",
    "\n",
    "    # Step 2: Fetch the latest email\n",
    "    latest_email = get_latest_email(service)\n",
    "    if not latest_email:\n",
    "        return\n",
    "\n",
    "    # Step 3: Extract email details\n",
    "    email_details = extract_email_details(latest_email)\n",
    "    print(\"Latest Email Details:\")\n",
    "    print(json.dumps(email_details, indent=4))\n",
    "\n",
    "    # Step 4: Classify the email body\n",
    "    email_body = email_details.get('Body', '')\n",
    "    if not email_body:\n",
    "        print(\"No body content to classify.\")\n",
    "        classification = 'No Body Content'\n",
    "    else:\n",
    "        classification = classify_email_body(email_body)\n",
    "        print(f\"Classification: {classification}\")\n",
    "\n",
    "    # Step 5: Update email details with classification\n",
    "    email_details['Classification'] = classification\n",
    "    # Example: Save to 'classified_email.json'\n",
    "    with open('classified_email.json', 'w') as f:\n",
    "        json.dump(email_details, f, indent=4)\n",
    "    print(\"Email classification saved to 'classified_email.json'.\")\n",
    "\n",
    "    # Step 6: Generate reply based on classification\n",
    "    if classification in ['FAQ', 'Inquiry']:\n",
    "        # Initialize RAG system\n",
    "        qa_chain = setup_rag()\n",
    "        if qa_chain:\n",
    "            # Use the email body as the query\n",
    "            rag_response = generate_rag_response(qa_chain, email_body)\n",
    "            reply_body = rag_response\n",
    "            print(f\"Generated RAG Response: {reply_body}\")\n",
    "        else:\n",
    "            reply_body = \"Thank you for your email. We will review your message and respond accordingly.\"\n",
    "            print(\"RAG system not available. Using generic response.\")\n",
    "    else:\n",
    "        # For 'Other' or 'No Body Content', use predefined responses\n",
    "        if classification == 'No Body Content':\n",
    "            reply_body = \"Thank you for your email. It appears there was no content in your message. Please provide more details so we can assist you better.\"\n",
    "        else:\n",
    "            reply_body = \"Thank you for your email. We will review your message and respond accordingly.\"\n",
    "\n",
    "    # Step 7: Send the reply\n",
    "    send_reply(service, latest_email, reply_body)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
