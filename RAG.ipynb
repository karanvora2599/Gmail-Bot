{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(r\"C:\\Users\\karan\\Documents\\Projects\\Gmail\\nvidia-ai-aerial-faq.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=7500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings using Ollama\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize Ollama model\n",
    "llm = Ollama(model=\"llama3.2:latest\")\n",
    "\n",
    "# Create QA chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"vectorstore_directory\"  # Specify directory here\n",
    ")\n",
    "\n",
    "# Save to disk\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load the saved vector store\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"vectorstore_directory\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Initialize Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\")\n",
    "\n",
    "# Create the QA chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Query the document\n",
    "chat_history = []\n",
    "query = \"What is this document about?\"\n",
    "result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "\n",
    "# For follow-up questions, update chat history\n",
    "chat_history.append((query, result['answer']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is about NVIDIA AI Aerial FAQ. It's a set of Questions and Answers regarding the NVIDIA AI Aerial system, its deployment, and various use cases, including ARC-OTA and OAI stack.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Any, Optional, List\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "# Initialize Cerebras client\n",
    "client = Cerebras(\n",
    "    api_key=os.getenv(\"CEREBRAS_API_KEY\", \"csk-e2e8kypw838rwmpjxd9nx2vn5jrertm339fnrcnt9c6p8hmx\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class CloudLLM(LLM):\n",
    "    _client: Any = PrivateAttr()\n",
    "    model_name: str\n",
    "\n",
    "    def __init__(self, client: Any, model_name: str):\n",
    "        super().__init__(model_name=model_name)\n",
    "        self._client = client\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response = self._client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=self.model_name,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"cloud_llm\"\n",
    "\n",
    "\n",
    "# Load the saved vector store\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"vectorstore_directory\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Initialize the custom cloud LLM\n",
    "cloud_llm = CloudLLM(client=client, model_name=\"llama3.1-8b\")\n",
    "\n",
    "# Create the conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cloud_llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Query the document\n",
    "chat_history = []\n",
    "query = \"What is this document about?\"\n",
    "result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "\n",
    "# Update chat history for follow-up questions\n",
    "chat_history.append((query, result['answer']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Email Details:\n",
      "{\n",
      "    \"Subject\": \"Test Inquiry\",\n",
      "    \"From\": \"Karan Vora <kv2154@nyu.edu>\",\n",
      "    \"To\": \"Karan Vora <karanvora210@gmail.com>\",\n",
      "    \"Body\": \"I want to inquiry about the specifications of the Nvidia Ariel Products and\\r\\nits usecases\\r\\n\\r\\nBest Regards,\\r\\nKaran Vora\\r\\nNew York University Class of 2024\\r\\nMS Computer Engineering\\r\\nkv2154@nyu.edu\\r\\nkaranvora210@gmail.com\\r\\nContact Number: +1 929-930-0027\\r\\nLinkedin <https://www.linkedin.com/in/karan-vora-574961188/>\\r\\n\"\n",
      "}\n",
      "Classification: Inquiry\n",
      "Email classification saved to 'classified_email.json'.\n",
      "Generated RAG Response: Subject: Inquiry about NVIDIA AI Aerial Specifications and Usecases\n",
      "\n",
      "Dear Karan Vora,\n",
      "\n",
      "Thank you for reaching out to us about NVIDIA AI Aerial Specifications and Usecases. \n",
      "\n",
      "To address your inquiry, we'll provide relevant information based on the NVIDIA AI Aerial FAQ document. \n",
      "\n",
      "1. **NVIDIA AI Aerial Components, Interfaces, and Usecases:**\n",
      "   - cuPHY (PHY components) and cuMAC (MAC components) are the underlying frameworks used in NVIDIA AI Aerial.\n",
      "   - For AI components, Torch is used, and Sionna allows the creation of designs for the data plane by providing a link-level environment.\n",
      "   - NVIDIA AI Aerial can be used for simulating metamaterial surfaces, channel estimation, and other wireless communication tasks.\n",
      "\n",
      "2. **Specifications and Limitations:**\n",
      "   - The maximum length supported in vectorization is not explicitly stated but is compared to AVX 512.\n",
      "   - The GPU and CPU do not share the same system clock, and synchronization between L2 (CPU) and L1 (GPU) is handled separately.\n",
      "\n",
      "3. **Usecases and Applications:**\n",
      "   - NVIDIA AI Aerial can be used for various usecases, including 6G frequency bands, mmWave direct Line Of Sight connections, and more accurate channel estimation models.\n",
      "   - The EM model has no issues above 100MHz, but the accuracy of the map may vary at different bands.\n",
      "\n",
      "4. **Technical Specifications:**\n",
      "   - Each A100/H100 Streaming Multiprocessor (SM) can execute 4 concurrent warps (32 threads) and supports SIMT (Single Instruction Multiple Threads) execution.\n",
      "   - The H100 GPU has 132 such SMs.\n",
      "\n",
      "Please note that some of the information might not be explicitly mentioned in the provided document. If you have further questions or need more specific information, please feel free to ask.\n",
      "\n",
      "Best Regards,\n",
      "NVIDIA AI Aerial Team\n",
      "Reply sent to thread ID: 194516f0df2c4c85\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import base64\n",
    "import json\n",
    "from email.mime.text import MIMEText\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Any, Optional, List\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "# -------------------- Gmail API Setup --------------------\n",
    "\n",
    "# Define the scope for Gmail API\n",
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/gmail.readonly',\n",
    "    'https://www.googleapis.com/auth/gmail.send'\n",
    "]\n",
    "\n",
    "def authenticate_gmail():\n",
    "    \"\"\"Authenticate the user and return the Gmail service.\"\"\"\n",
    "    creds = None\n",
    "    # Load credentials from token.pickle if it exists\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If no valid credentials, prompt the user to log in\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secret.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0, access_type='offline', prompt='consent')\n",
    "        # Save the credentials for future use\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    # Build the Gmail service\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "    return service\n",
    "\n",
    "def get_latest_email(service):\n",
    "    \"\"\"Retrieve the most recent email from the user's Primary inbox.\"\"\"\n",
    "    try:\n",
    "        # Fetch the latest email from the Primary category\n",
    "        results = service.users().messages().list(\n",
    "            userId='me',\n",
    "            maxResults=1,\n",
    "            q=\"category:primary\"\n",
    "        ).execute()\n",
    "        messages = results.get('messages', [])\n",
    "        if not messages:\n",
    "            print(\"No messages found in the Primary category.\")\n",
    "            return None\n",
    "        # Get the message details\n",
    "        message = service.users().messages().get(\n",
    "            userId='me',\n",
    "            id=messages[0]['id'],\n",
    "            format='full'\n",
    "        ).execute()\n",
    "        return message\n",
    "    except Exception as error:\n",
    "        print(f\"An error occurred while fetching the latest email: {error}\")\n",
    "        return None\n",
    "\n",
    "def decode_message_body(encoded_body):\n",
    "    \"\"\"Decode the base64url-encoded message body.\"\"\"\n",
    "    try:\n",
    "        # Add padding if necessary\n",
    "        padding = 4 - (len(encoded_body) % 4)\n",
    "        if padding:\n",
    "            encoded_body += \"=\" * padding\n",
    "        decoded_bytes = base64.urlsafe_b64decode(encoded_body)\n",
    "        return decoded_bytes.decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding message body: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_email_details(message):\n",
    "    \"\"\"Extract details from the email message.\"\"\"\n",
    "    headers = message['payload'].get('headers', [])\n",
    "    subject = next((header['value'] for header in headers if header['name'] == 'Subject'), \"No Subject\")\n",
    "    sender = next((header['value'] for header in headers if header['name'] == 'From'), \"Unknown Sender\")\n",
    "    recipient = next((header['value'] for header in headers if header['name'] == 'To'), \"Unknown Recipient\")\n",
    "    body = \"\"\n",
    "    if 'parts' in message['payload']:\n",
    "        for part in message['payload']['parts']:\n",
    "            if part['mimeType'] == 'text/plain' and 'data' in part['body']:\n",
    "                body = decode_message_body(part['body']['data'])\n",
    "                break\n",
    "    elif 'body' in message['payload'] and 'data' in message['payload']['body']:\n",
    "        body = decode_message_body(message['payload']['body']['data'])\n",
    "    return {\n",
    "        \"Subject\": subject,\n",
    "        \"From\": sender,\n",
    "        \"To\": recipient,\n",
    "        \"Body\": body\n",
    "    }\n",
    "\n",
    "def create_reply_message(service, original_message, reply_body):\n",
    "    \"\"\"Create a reply message for the given original email.\"\"\"\n",
    "    try:\n",
    "        # Extract necessary details from the original message\n",
    "        thread_id = original_message['threadId']\n",
    "        message_id = None\n",
    "        headers = original_message['payload'].get('headers', [])\n",
    "        for header in headers:\n",
    "            if header['name'] == 'Message-ID':\n",
    "                message_id = header['value']\n",
    "            elif header['name'] == 'From':\n",
    "                sender = header['value']\n",
    "            elif header['name'] == 'To':\n",
    "                recipient = header['value']\n",
    "            elif header['name'] == 'Subject':\n",
    "                subject = header['value']\n",
    "\n",
    "        # Create the reply email\n",
    "        reply = MIMEText(reply_body)\n",
    "        reply['To'] = sender  # Reply to the original sender\n",
    "        reply['From'] = recipient  # Your email address\n",
    "        reply['Subject'] = f\"Re: {subject}\"\n",
    "        reply['In-Reply-To'] = message_id\n",
    "        reply['References'] = message_id\n",
    "\n",
    "        # Encode the message\n",
    "        raw_message = base64.urlsafe_b64encode(reply.as_bytes()).decode()\n",
    "\n",
    "        # Create the message object with threadId\n",
    "        return {\n",
    "            'raw': raw_message,\n",
    "            'threadId': thread_id\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(f\"An error occurred while creating the reply message: {error}\")\n",
    "        return None\n",
    "\n",
    "def send_reply(service, original_message, reply_body):\n",
    "    \"\"\"Send a reply to an email message.\"\"\"\n",
    "    try:\n",
    "        reply = create_reply_message(service, original_message, reply_body)\n",
    "        if reply:\n",
    "            sent_message = service.users().messages().send(userId='me', body=reply).execute()\n",
    "            print(f\"Reply sent to thread ID: {sent_message['threadId']}\")\n",
    "        else:\n",
    "            print(\"Failed to create reply message.\")\n",
    "    except Exception as error:\n",
    "        print(f\"An error occurred while sending the reply: {error}\")\n",
    "\n",
    "# -------------------- Cerebras API Setup --------------------\n",
    "\n",
    "# Initialize the Cerebras client\n",
    "CEREBRAS_API_KEY = os.getenv(\"CEREBRAS_API_KEY\", \"csk-e2e8kypw838rwmpjxd9nx2vn5jrertm339fnrcnt9c6p8hmx\")  # Replace with your actual API key or set as environment variable\n",
    "client = Cerebras(api_key=CEREBRAS_API_KEY)\n",
    "\n",
    "def classify_email_body(email_body):\n",
    "    \"\"\"Classify the email body as 'FAQ', 'Inquiry', or 'Other'.\"\"\"\n",
    "    try:\n",
    "        # Define the system prompt\n",
    "        system_prompt = \"\"\"You are an email classifier. Classify the following email body into one of the following categories: 'FAQ', 'Inquiry', or 'Other'. Respond with the classification only.\"\"\"\n",
    "        \n",
    "        # Create the completion request\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b\",  # Specify the model you wish to use\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": email_body}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=10,\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "\n",
    "        # Extract and return the classification\n",
    "        parsed_content = completion.choices[0].message.content.strip()\n",
    "        return parsed_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying email body: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# -------------------- RAG (Retrieval-Augmented Generation) Setup --------------------\n",
    "\n",
    "class CloudLLM(LLM):\n",
    "    _client: Any = PrivateAttr()\n",
    "    model_name: str\n",
    "\n",
    "    def __init__(self, client: Any, model_name: str):\n",
    "        super().__init__(model_name=model_name)\n",
    "        self._client = client\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        try:\n",
    "            response = self._client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=self.model_name,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response with LLM: {e}\")\n",
    "            return \"I'm sorry, I couldn't process your request at this time.\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"cloud_llm\"\n",
    "\n",
    "def setup_rag():\n",
    "    \"\"\"Initialize the RAG system.\"\"\"\n",
    "    try:\n",
    "        # Load the saved vector store\n",
    "        embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=\"vectorstore_directory\",  # Ensure this directory exists and is populated\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        # Initialize the custom cloud LLM\n",
    "        cloud_llm = CloudLLM(client=client, model_name=\"llama3.1-8b\")  # Ensure the model name is correct and available\n",
    "\n",
    "        # Create the conversational retrieval chain\n",
    "        qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=cloud_llm,\n",
    "            retriever=vectorstore.as_retriever()\n",
    "        )\n",
    "\n",
    "        return qa_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up RAG system: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_rag_response(qa_chain, query, chat_history=None):\n",
    "    \"\"\"Generate a response using the RAG system.\"\"\"\n",
    "    try:\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "        result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "        return result['answer']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating RAG response: {e}\")\n",
    "        return \"I'm sorry, I couldn't find the information you're looking for.\"\n",
    "\n",
    "# -------------------- Main Workflow --------------------\n",
    "\n",
    "def main():\n",
    "    # Step 1: Authenticate and build the Gmail service\n",
    "    service = authenticate_gmail()\n",
    "\n",
    "    # Step 2: Fetch the latest email\n",
    "    latest_email = get_latest_email(service)\n",
    "    if not latest_email:\n",
    "        return\n",
    "\n",
    "    # Step 3: Extract email details\n",
    "    email_details = extract_email_details(latest_email)\n",
    "    print(\"Latest Email Details:\")\n",
    "    print(json.dumps(email_details, indent=4))\n",
    "\n",
    "    # Step 4: Classify the email body\n",
    "    email_body = email_details.get('Body', '')\n",
    "    if not email_body:\n",
    "        print(\"No body content to classify.\")\n",
    "        classification = 'No Body Content'\n",
    "    else:\n",
    "        classification = classify_email_body(email_body)\n",
    "        print(f\"Classification: {classification}\")\n",
    "\n",
    "    # Step 5: Update email details with classification\n",
    "    email_details['Classification'] = classification\n",
    "    # Example: Save to 'classified_email.json'\n",
    "    with open('classified_email.json', 'w') as f:\n",
    "        json.dump(email_details, f, indent=4)\n",
    "    print(\"Email classification saved to 'classified_email.json'.\")\n",
    "\n",
    "    # Step 6: Generate reply based on classification\n",
    "    if classification in ['FAQ', 'Inquiry']:\n",
    "        # Initialize RAG system\n",
    "        qa_chain = setup_rag()\n",
    "        if qa_chain:\n",
    "            # Use the email body as the query\n",
    "            rag_response = generate_rag_response(qa_chain, email_body)\n",
    "            reply_body = rag_response\n",
    "            print(f\"Generated RAG Response: {reply_body}\")\n",
    "        else:\n",
    "            reply_body = \"Thank you for your email. We will review your message and respond accordingly.\"\n",
    "            print(\"RAG system not available. Using generic response.\")\n",
    "    else:\n",
    "        # For 'Other' or 'No Body Content', use predefined responses\n",
    "        if classification == 'No Body Content':\n",
    "            reply_body = \"Thank you for your email. It appears there was no content in your message. Please provide more details so we can assist you better.\"\n",
    "        else:\n",
    "            reply_body = \"Thank you for your email. We will review your message and respond accordingly.\"\n",
    "\n",
    "    # Step 7: Send the reply\n",
    "    send_reply(service, latest_email, reply_body)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
